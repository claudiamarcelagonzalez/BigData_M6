---
title: "Análisis de la producción científica de la base de datos Scopus indizada bajo la categoría temática Aquatic Science"
subtitle: <strong>¿Qué tan representativa es de la producción científica sobre recursos pesqueros?</strong><br><br>  
author: <strong>Claudia M. González</strong>  -  IdIHCS (CONICET/UNLP)
date: diciembre de 2019
output:
 html_document:
  code_folding: hide 
  toc: true
  toc_float: true
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(cache=FALSE, echo=TRUE, include=TRUE, eval=TRUE, results="hide", comment=NA, warning = FALSE, message=FALSE)
```

## Introducción

Las bases de datos bibliográficas [Scopus](https://www.elsevier.com/solutions/scopus) y [Web of Science](http://login.webofknowledge.com) son las fuentes más utilizadas para la realización de estudios bibliométricos sobre producción científica. Esto se debe a que ambas fuentes son multidisciplinares, cuentan con una interfaz de búsqueda potente que permite el planteo de estrategias de búsqueda avanzadas, además de presentar buenos niveles de normalización de datos y diferentes formatos de descarga. Si bien se sabe que la totalidad de la producción científica de cualquier agregado que se tome para realizar un estudio (país, disciplina, institución,etc.) no está contenida en esas bases de datos en su totalidad, también se sabe que no hay ninguna otra fuente alternativa que posea toda la cobertura. En el caso de la realización de estudios sobre el agregado Argentina, es más común utilizar la base de datos Scopus ya que, aún siendo un producto comercial de la empresa Elsevier, se la puede consultar libremente dentro de la red de instituciones de la Secretaría de Estado de Ciencia, Tecnología e Innovación productiva a través de la Biblioteca Electrónica.

La base de datos Scopus agrupa las revistas que indiza bajo 334 categorías codificadas que se encuentran explícitas en su [Scopus Subject Areas and All Science Journal Classification Codes (ASJC)](https://service.elsevier.com/app/answers/detail/a_id/15181/supporthub/scopus/). Dado que la base de datos no realiza una clasificación temática a nivel de artículo, en los estudios bibliométricos es común tomar la misma categoría temática que se le ha asignado a la revista como la representativa para cada uno de sus artículos. 

Por lo tanto, cuando se desea realizar estudios de producción científica por agregados temáticos, se identifica dentro de las 334 categorías la que se desea, o bien se realiza un agrupamiento de categorías que se considera que representan a un área temática que no ha sido definida como tal dentro de la ASJC. Tal es el caso de interés en la investigación dentro de la cual este trabajo se inscribe, en la cual se busca analizar la producción científica Argentina en el tema <strong>Agroindustria</strong>. Dado que la categoría Agroindustria no está definida en la ASJC, se explora la posibilidad de abordarla a partir de la sumatoria de la producción de las categorías **Agronomy and Crop Science** (1102) ; **Food Science** (1106) ; **Food Animals** (3403) ; **Acuatic Science** (1104) ; **Forestry** (1107) ; **Horticulture** (1108). 

Sin embargo, un estudio anterior realizado sobre la producción científica Argentina en el periodo 2007-2016 sobre temas locales, en el que se aplicó como criterio de selección temática el abordaje anterior, (ver el trabajo [¿Responde la producción científica argentina a patrones de regionalización o globalización?](http://rpubs.com/cgonzalez/516773) ), nos hace dudar sobre lo que representa la categoría <strong>Aquatic Science</strong> en la conformación de la nueva categoría Agroindustria. ¿Por qué la duda? Si se observa en los resultados de dicho trabajo, vemos que Aquatic Science tiene un protagonismo significativo, lo que lleva a pensar que dicha categoría agrupa producción relacionada con recursos pesqueros y su explotación - pertinente para nuestro estudio-, pero además, también incluye producción que corresponde más a la ciencia básica zoológica, de vinculación no tan directa con la temática Agroindustria.    

## Objetivo

El objetivo de este trabajo es identificar los tópicos que tratan una porción de artículos del estudio anterior, en este caso solo los correspondientes a temas locales de la categoria Scopus Aquatic Science. El ánalisis se realiza sobre los títulos, resúmenes y palabras claves de los registros bibliográficos. De esta manera se busca obtener nueva evidencia que permita discernir sobre la validez o no de incluir la totalidad de la producción indizada bajo dicha categoría como parte de la producción de una gran área denominada Agroindustria.

## Materiales y métodos

Para el desarrollo del trabajo se formó un corpus de registros bibliográficos descargados de la base de datos [Scopus](https://www.elsevier.com/solutions/scopus). Tal como se indica en el [trabajo anterior](http://rpubs.com/cgonzalez/516773), en su selección se aplicó el rango temporal 2007-2016 como año de publicación, que en el título, resumen o palabras claves apareciera mencionado el topónimo Argentina o el nombre de alguna de las provincias argentinas, y que perteneciera a la categoría temática Aquatic Science (cod. 1104). Dicho corpus significa 2574 registros bibliográficos. Se procedió a descargarlos en formato BibTex. Es el formato elegido para poder convertirlos con facilidad en un **data frame** usando la función **convert2df** del paquete **Bibliometrix**. De ellos se tomó únicamente los datos de título, palabras claves y resumen, que son los que concentran la carga semántica de la referencia.
```{r}
library (tidyverse)
library (bibliometrix)

Aquatic_LxArg_LxEx <- as.tibble(convert2df(file = '/Users/claudiamgonzalez/Documents/FORM/BigData_Territorio/Modulo6_Machine_Learning/TrabajoFinal/BigData_M6_PROY/Datos/scopus_Acuatic_LxArg_LxEx.bib', dbsource = "scopus", format = "bibtex"))

#Selecciono solo los campos título, resumen y palabras claves y los agrupo en una sola variable denominada texto.
Aquatic_LxArg_LxEx <- Aquatic_LxArg_LxEx %>%
  select(TI,DE,ID,AB) %>%
  unite(text, TI,DE,ID,AB, sep = " ")

#Le agrego el id de documento
Aquatic_LxArg_LxEx <- data.frame(doc_id=row.names(Aquatic_LxArg_LxEx),
                                 text=Aquatic_LxArg_LxEx$text)
```

Se realiza un preprocesamiento de los textos transformándolos a mayúsculas, removiendo la puntuación, los números y aplicando una *stopword* para el idioma inglés. Para ello se usa el paquete **tm**.

```{r}
library (tm)

#Creo el Corpus
Aquatic_LxArg_LxEx_pre_Corpus = Corpus(VectorSource(Aquatic_LxArg_LxEx$text))

#Limpio el corpus
Aquatic_LxArg_LxEx_pre_Corpus = tm_map(Aquatic_LxArg_LxEx_pre_Corpus, content_transformer(tolower))
Aquatic_LxArg_LxEx_pre_Corpus = tm_map(Aquatic_LxArg_LxEx_pre_Corpus, removePunctuation)
Aquatic_LxArg_LxEx_pre_Corpus = tm_map(Aquatic_LxArg_LxEx_pre_Corpus, removeNumbers)
Aquatic_LxArg_LxEx_pre_Corpus = tm_map(Aquatic_LxArg_LxEx_pre_Corpus, removeWords, stopwords(kind = "en"))

# Aplico stemming
Aquatic_LxArg_LxEx_pre_Corpus <- tm_map(Aquatic_LxArg_LxEx_pre_Corpus,stemDocument) 

# Elimino los caracteres \n
Aquatic_LxArg_LxEx_pre_Corpus = tm_map(Aquatic_LxArg_LxEx_pre_Corpus, content_transformer(function(x) str_replace_all(x, pattern = '\n',replacement = ' ')))
```

Se genera la matriz documento-término que permite tratar al corpus como una bolsa de palabras (BoW) y determinar sus frecuencias. 

```{r}
#Creo la matriz documento-término
Aquatic_LxArg_LxEx_DTM = DocumentTermMatrix(Aquatic_LxArg_LxEx_pre_Corpus, control = list(minWordLength = 1))
```

Aplico el algoritmo LDA perteneciente a los conocidos "Topic Models".

```{r}
library(topicmodels)
library(LDAvis)

ui = unique(Aquatic_LxArg_LxEx_DTM$i)
Aquatic_LxArg_LxEx_DTM = Aquatic_LxArg_LxEx_DTM[ui,]

dim(Aquatic_LxArg_LxEx_DTM)

lda_fit_2 <- LDA(Aquatic_LxArg_LxEx_DTM, k = 2, method = "Gibbs", control = list(delta=0.6,seed = 1234))

saveRDS(lda_fit_2,'Resultados/lda_fit_2.rds')

lda_fit_2 <- read_rds('Resultados/lda_fit_2.rds')
#lda_fit

Topics_2 <- terms(lda_fit_2, 20)
Topics_2

lda_fit_5 <- LDA(Aquatic_LxArg_LxEx_DTM, k = 5, method = "Gibbs", control = list(delta=0.6,seed = 1234))

saveRDS(lda_fit_5,'Resultados/lda_fit_5.rds')

lda_fit_5 <- read_rds('Resultados/lda_fit_5.rds')
#lda_fit

Topics_5 <- terms(lda_fit_5, 10)
Topics_5

lda_fit_10 <- LDA(Aquatic_LxArg_LxEx_DTM, k = 10, method = "Gibbs", control = list(delta=0.6,seed = 1234))

saveRDS(lda_fit_10,'Resultados/lda_fit_10.rds')

lda_fit_10 <- read_rds('Resultados/lda_fit_10.rds')
#lda_fit

Topics_10 <- terms(lda_fit_10, 10)
Topics_10

lda_fit_15 <- LDA(Aquatic_LxArg_LxEx_DTM, k = 15, method = "Gibbs", control = list(delta=0.6,seed = 1234))

saveRDS(lda_fit_15,'Resultados/lda_fit_15.rds')

lda_fit_15 <- read_rds('Resultados/lda_fit_15.rds')
#lda_fit

Topics_15 <- terms(lda_fit_15, 15)
Topics_15

lda_fit_20 <- LDA(Aquatic_LxArg_LxEx_DTM, k = 20, method = "Gibbs", control = list(delta=0.6,seed = 1234))

saveRDS(lda_fit_20,'Resultados/lda_fit_20.rds')

lda_fit_20 <- read_rds('Resultados/lda_fit_20.rds')
#lda_fit

Topics_20 <- terms(lda_fit_20, 20)
Topics_20
```

Topic_5


Se realiza un proceso de clusterización ...

```{r}
# Se vuelve a convertir el corpus en data frame para poder aplicar clustering con 
Aquatic_LxArg_LxEx_pre_Corpus_df <-           data.frame(text=unlist(sapply(Aquatic_LxArg_LxEx_pre_Corpus, `[`, "content")), stringsAsFactors=F)

# Se hacen los clusters
Aquatic_LxArg_LxEx_CLUS_8 <- conceptualStructure(Aquatic_LxArg_LxEx_pre_Corpus_df, field="text", stemming=FALSE, minDegree=100, k.max = 8, labelsize=4)
```





```{r}
library(textmineR)

# load nih_sample data set from textmineR
data(Aquatic_LxArg_LxEx)

# create a document term matrix 
dtm <- CreateDtm(doc_vec = Aquatic_LxArg_LxEx$text, # character vector of documents
                 doc_names = Aquatic_LxArg_LxEx$doc_id, # document names
                 ngram_window = c(1, 2), # minimum and maximum n-gram length
                 stopword_vec = c(stopwords::stopwords("en"), # stopwords from tm
                                  stopwords::stopwords(source = "smart")), # this is the default value
                 lower = TRUE, # lowercase - this is the default value
                 remove_punctuation = TRUE, # punctuation - this is the default
                 remove_numbers = TRUE, # numbers - this is the default
                 verbose = FALSE, # Turn off status bar for this demo
                 cpus = 2) # default is all available cpus on the system

# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)
```

```{r}
# TF-IDF and cosine similarity
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf

tfidf <- t(tfidf)
```

```{r}
csim <- tfidf / sqrt(rowSums(tfidf * tfidf))

csim <- csim %*% t(csim)
```


```{r}
cdist <- as.dist(1 - csim)
```

```{r}
truth.K <- 2
```

```{r}
clustering.kmeans <- kmeans(tfidf,2) 
clustering.hierarchical <- hclust(cdist, method = "ward.D2") 
clustering.dbscan <- dbscan::hdbscan(cdist, minPts = 10) 
```


```{r}
hc <- hclust(cdist, "ward.D")

clustering <- cutree(hc, 10)

# plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
   #  ylab = "", xlab = "", yaxt = "n")

# rect.hclust(hc, 10, border = "red")

```

```{r}
p_words <- colSums(dtm) / sum(dtm)

cluster_words <- lapply(unique(clustering), function(x){
  rows <- dtm[ clustering == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows <- rows[ , colSums(rows) > 0 ]
  
  colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})
```

```{r}
# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
                              size = as.numeric(table(clustering)),
                              top_words = sapply(cluster_words, function(d){
                                paste(
                                  names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                  collapse = ", ")
                              }),
                              stringsAsFactors = FALSE)
```


```{r}
library (tm)

#Creo el Corpus
Aquatic_LxArg_LxEx_pre_Corpus = Corpus(VectorSource(Aquatic_LxArg_LxEx))

#Limpio el corpus
Aquatic_LxArg_LxEx_pre_Corpus = tm_map(Aquatic_LxArg_LxEx_pre_Corpus, content_transformer(tolower))
Aquatic_LxArg_LxEx_pre_Corpus = tm_map(Aquatic_LxArg_LxEx_pre_Corpus, removePunctuation)
Aquatic_LxArg_LxEx_pre_Corpus = tm_map(Aquatic_LxArg_LxEx_pre_Corpus, removeNumbers)
Aquatic_LxArg_LxEx_pre_Corpus = tm_map(Aquatic_LxArg_LxEx_pre_Corpus, removeWords, stopwords(kind = "en"))
```





Hago clustering

```{r}
# El paquete corpus me permite convertir el corpus de TM en data Frame
library(qdap)
Aquatic_LxArg_LxEx_pre_Corpus_df <- as.data.frame(Aquatic_LxArg_LxEx_pre_Corpus) 

#Aquatic_LxArg_LxEx_pre_Corpus_df <- DataframeSource(Aquatic_LxArg_LxEx_pre_Corpus) 



library(qdap); as.data.frame(mycorpus)
 

Aquatic_LxArg_LxEx_pre_Corpus_df <- as.data.frame(text=unlist(sapply(Aquatic_LxArg_LxEx_pre_Corpus, `[[`, "content")), 
    stringsAsFactors=F)

df <- data.frame(text = get("content", Aquatic_LxArg_LxEx_pre_Corpus))

dataframe <- data.frame(text=unlist(sapply(mycorpus, `[`, "content")), 
    stringsAsFactors=F)

Aquatic_LxArg_LxEx_pre_Corpus_df <- data.frame(text = get("content", Aquatic_LxArg_LxEx_pre_Corpus))


library (corpus)

Aquatic_LxArg_LxEx_pre_Corpus_df <- as_corpus_frame(Aquatic_LxArg_LxEx_pre_Corpus)

# Genero los clusters

Aquatic_LxArg_LxEx_CLUS_8 <- conceptualStructure(Aquatic_LxArg_LxEx_pre_Corpus_df, field="text", stemming=FALSE, minDegree=100, k.max = 8, labelsize=4)

```




Aplico Topics models
```{r}
library(topicmodels)
library(LDAvis)
library(tsne)

ui = unique(Aquatic_LxArg_LxEx_DTM$i)
dtm = Aquatic_LxArg_LxEx_DTM[ui,]

dim(Aquatic_LxArg_LxEx_DTM)

dim(dtm)

lda_fit <- LDA(dtm, k = 10,method = "Gibbs", control = list(delta=0.6,seed = 1234))
saveRDS(lda_fit,'lda_fit.rds')

lda_fit <- read_rds('lda_fit.rds')
lda_fit

Terms <- terms(lda_fit, 10)
Terms

topicmodels_json_ldavis <- function(fitted, dtm){
  svd_tsne <- function(x) tsne(svd(x)$u)
  
  # Find required quantities
  phi <- as.matrix(posterior(fitted)$terms)
  theta <- as.matrix(posterior(fitted)$topics)
  vocab <- colnames(phi)
  term_freq <- slam::col_sums(dtm)
  
  # Convert to json
  json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
                                 vocab = vocab,
                                 mds.method = svd_tsne,
                                 plot.opts = list(xlab="tsne", ylab=""),
                                 doc.length = as.vector(table(dtm$i)),
                                 term.frequency = term_freq)
  
  return(json_lda)
}

json_res <- topicmodels_json_ldavis(lda_fit, dtm)

serVis(json_res)

```



## Resultados

## Conclusión



```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tm)
library(wordcloud2)
library(topicmodels)
library(LDAvis)
library(tsne)



```

df <- read_rds()
df

### 1. Limpieza y organización de la información

#### 1.a Crear un objeto Corpus de la librería `tm`

```{r}
#
```

#### 1.b Limpiar el Corpus con `tm_map`

- Pasar a minúscula
- Eliminar puntuación
- Eliminar numeros
- Eliminar stopwords

```{r}
#
```

Inspeccionar el corpus y revisar si quedaron Caracteres especiales que deban ser eliminados. En caso de que así fuera eliminarlos con `tm_map` y `content_transformer(function(x) str_remove_all(x, pattern = ))`

```{r}
#
```

#### 1.c Crear una matriz Documento-término con `DocumentTermMatrix`

```{r}
#
```

### 2. Wordcloud

#### 2.a Buscar los términos más frecuentes con `findMostFreqTerms`


```{r}
#
```

#### 2.b Crear un dataframe con las palabras más frecuentes

```{r}
#
```

#### 2.c Crear una nube de palabras con `wordcloud2`

```{r}
#
```


### 3. Topic Modelling

#### 3.1  eliminar los documentos vacíos de la matriz documento-término

```{r}
#
```

#### 3.2 Entrenar un modelo de LDA con la función `LDA`

```{r}
#
```

#### 3.3 Recuperar los diez términos más frecuentes de cada Tópico con `terms`

```{r}
#
```

